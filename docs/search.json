[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HOME",
    "section": "",
    "text": "Email\n LinkedIn\n Scholar\n GitHub\n ORCID\n\n\nMy name is Andrew M. Demetriou, and I work on Trustworthy and Responsible Artificial Intelligence.\nI aim to bring more thought to how we measure the performance of AI systems, by integrating knowledge from the social sciences, and the science of measurement to ‚Äòground-truthing‚Äô - the process of designing, gathering, and evaluating the data we use to measure the performance of AI systems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "HOME",
    "section": "",
    "text": "Andrew M. Demetriou\n\nIntelligent Systems PhD Candidate\n\n\nDelft University of Technology\n Email\n LinkedIn\n Scholar\n GitHub\n ORCID\n\n\n\nAs a Trustworthy Artificial Intelligence (AI) systems as a PhD candidate in the Intelligent Systems department at Delft University of Technology, in the lab of Cynthia C. S. Liem, I study how we design, collect, and analyze the data we use to evaluate AI systems - often called the ‚ÄúGold Standard‚Äù or the ‚ÄúGround Truth‚Äù. As a PhD candidate, I bring in knowledge from the Social Sciences, particularly Psychometrics, to critique and inform best practices for how to go about the process of ‚ÄúGround Truthing‚Äù.\n\n\nEducation\n PhD in Computer Science,\n(ABD) Delft University of Technology\n Research MSc in Psychology,\nVrije Universiteit Amsterdam, Cum Laude\n BA in Philosphy and Political Science, CUNY Queens College"
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "Andrew M. Demetriou",
    "section": "",
    "text": "Andrew M. Demetriou is researching Trustworthy Artificial Intelligence (AI) systems as a PhD candidate in the Intelligent Systems department at Delft University of Technology. He works as part of a lab aimed at increasing the trustworthiness of AI technology, focusing on how to gather and evaluate the quality of data used as Ground Truth in AI systems. Specifically, he applies knowledge from the fields of Psychometrics and Survey Science to develop reproducible data gathering and evaluation procedures from participants.\n\n\n\n\n\nResponsible AI Systems\n\nGround Truthing for AI\n\nAI for Social Science\n\nOpen Science\n\n\n\n\n\n\n PhD in Artificial Intelligence,\n(ABD) Delft University of Technology\n Research MSc in Psychology,\nVrije Universiteit Amsterdam, Cum Laude"
  },
  {
    "objectID": "licenses.html",
    "href": "licenses.html",
    "title": "Licensing",
    "section": "",
    "text": "üìú Licensing This repository is dual-licensed to support both code reuse and responsible content sharing:\nüßë‚Äçüíª Code (e.g.¬†R scripts, Quarto setup) is licensed under the MIT License. ‚Üí You are free to use, modify, and distribute the code, as long as attribution is given.\nüìÑ Written content (e.g.¬†blog posts, project write-ups, thesis commentary) is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). ‚Üí This allows sharing and adaptation, with attribution.\nIf you‚Äôre unsure how to reuse any part of this project or you‚Äôd like to collaborate, feel free to open an issue or contact me by email."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "HOME",
    "section": "",
    "text": "Most widely consumed music in Western countries includes song lyrics, with US samples indicating that nearly all personal song libraries contain lyrics. Social science theory suggests that personal values expressed in widely consumed text, like song lyrics, may align with or diverge from those of the listener, potentially shaping their response to the music. This highlights the potential of automated value estimation in lyrics for downstream music information retrieval (MIR) tasks, like personalization. Like prior work, we adopt a perspectivist approach informed by social science theory to reliably collect annotations and assess their quality. We then use a novel approach to automatically estimate values in song lyrics using large language models (LLMs), combined with programmatic prompt optimizer DSPy, gathering multiple estimates from each LLM to estimate intra-model reliability. We then compare aggregated human ratings with aggregated ratings from a subset of the most reliable LLMs, showing promising initial results. At the same time, in response to concerns about the energy footprint of LLMs, we monitor energy usage. We conclude that the results are a positive step forward in terms of estimating values in lyrics, but open challenges remain on the side of reliability and power requirements."
  },
  {
    "objectID": "projects.html#towards-estimating-values-from-lyrics-cultural-meaning-at-scale",
    "href": "projects.html#towards-estimating-values-from-lyrics-cultural-meaning-at-scale",
    "title": "Projects",
    "section": "",
    "text": "View on GitHub\n\n\n\n\nThis project investigates how cultural values manifest in popular music lyrics, using a hybrid of computational social science, word embeddings, and music information retrieval. We developed a novel method to estimate value orientations (e.g., self-direction vs.¬†conformity) from song lyrics across time, genres, and regions.\n\nüìä Built reproducible evaluation pipelines in R and Python\n\nüéØ Designed proxy tasks to validate model alignment with human value ratings\n\nüß† Co-authored the conceptual framework + led data wrangling and visualization\n\nüìç Submission under review at ISMIR 2025"
  },
  {
    "objectID": "projects.html#towards-estimating-values-from-lyrics",
    "href": "projects.html#towards-estimating-values-from-lyrics",
    "title": "HOME",
    "section": "",
    "text": "Most widely consumed music in Western countries includes song lyrics, with US samples indicating that nearly all personal song libraries contain lyrics. Social science theory suggests that personal values expressed in widely consumed text, like song lyrics, may align with or diverge from those of the listener, potentially shaping their response to the music. This highlights the potential of automated value estimation in lyrics for downstream music information retrieval (MIR) tasks, like personalization. Like prior work, we adopt a perspectivist approach informed by social science theory to reliably collect annotations and assess their quality. We then use a novel approach to automatically estimate values in song lyrics using large language models (LLMs), combined with programmatic prompt optimizer DSPy, gathering multiple estimates from each LLM to estimate intra-model reliability. We then compare aggregated human ratings with aggregated ratings from a subset of the most reliable LLMs, showing promising initial results. At the same time, in response to concerns about the energy footprint of LLMs, we monitor energy usage. We conclude that the results are a positive step forward in terms of estimating values in lyrics, but open challenges remain on the side of reliability and power requirements."
  },
  {
    "objectID": "projects.html#towards-estimating-values-from-lyrics-1",
    "href": "projects.html#towards-estimating-values-from-lyrics-1",
    "title": "Projects",
    "section": "",
    "text": "Most widely consumed music in Western countries includes song lyrics, with US samples indicating that nearly all personal song libraries contain lyrics. Social science theory suggests that personal values expressed in widely consumed text, like song lyrics, may align with or diverge from those of the listener, potentially shaping their response to the music. This highlights the potential of automated value estimation in lyrics for downstream music information retrieval (MIR) tasks, like personalization. Like prior work, we adopt a perspectivist approach informed by social science theory to reliably collect annotations and assess their quality. We then use a novel approach to automatically estimate values in song lyrics using large language models (LLMs), combined with programmatic prompt optimizer DSPy, gathering multiple estimates from each LLM to estimate intra-model reliability. We then compare aggregated human ratings with aggregated ratings from a subset of the most reliable LLMs, showing promising initial results. At the same time, in response to concerns about the energy footprint of LLMs, we monitor energy usage. We conclude that the results are a positive step forward in terms of estimating values in lyrics, but open challenges remain on the side of reliability and power requirements."
  },
  {
    "objectID": "projects.html#position-stop-making-unscientific-agi-performance-claims",
    "href": "projects.html#position-stop-making-unscientific-agi-performance-claims",
    "title": "HOME",
    "section": "Position: Stop Making Unscientific AGI Performance Claims",
    "text": "Position: Stop Making Unscientific AGI Performance Claims\n\nICML‚Äô24\nDevelopments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a ‚Äòperfect storm‚Äô for observing ‚Äòsparks‚Äô of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are ‚Äòcaused‚Äô by the model‚Äôs understanding of underlying ‚Äòground truth‚Äô relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes."
  }
]