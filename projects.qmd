---
title: ""
page-layout: full
format: html
toc: false
---

<div style="text-align: center;">
## Towards Automatic Personal Value Estimation in Song Lyrics with Large Language Models
</div>
::::: {.columns .responsive-columns}

::: {.column width="50%"}

[ISMIR, 2025](https://ismir.net) (under review) · [Supplement](https://andrew0302.github.io/values_from_lyrics_ISMIR2025/) · [<i class="bi bi-github"></i>](https://github.com/andrew0302/values_from_lyrics_ISMIR2025)

<div style="text-align: justify;">
Popular Western music almost always includes lyrics. In this work, we 'ground' personal values in lyrics, using theory from the Social Sciences. We then evaluate a group of language models on how well they estimate values in the lyrics. Figure shows pearson correlations between mean ratings of participants from a US sample stratified by ethnicity with mean ratings of 3 runs per model (gemma2:9b, phi4,qwen2.5:7b), by value. We show small to moderate positive correlations overall. 
</div>
:::

::: {.column width="50%" style="padding-left: 1rem;"}
<div style="background-color: white; padding: 0.25rem; border-radius: 0.25rem; margin-bottom: 0.5rem;">
![](images/wave_llm_participant_0na_byvalue_cor.png){width="100%" style="border-radius: 1rem; margin-bottom: 0.25rem;"}
</div>

:::
:::::
<div style="text-align: center;">
## Position: Stop Making Unscientific AGI Performance Claims
</div>
[ICML, 2024](https://dl.acm.org/doi/abs/10.5555/3692070.3692121) ·
[arXiv](https://arxiv.org/abs/2402.03962)

<div style="text-align: justify;">
Developments in AI and related fields, particularly large language models (LLMs), have created a 'perfect storm' for observing 'sparks' of Artificial General Intelligence (AGI) that are spurious. We argue and show that finding of meaningful patterns in the latent spaces of these models isn't evidence of AGI. Figure shows model outputs in response to sentences about inflation and deflation of prices (IP and DP), and inflation and deflation of birds (IB and DB). The vertical axis shows predicted inflation levels subtracted by the average predicted value of the probe for random noise. If the model had 'understood', we would expect a significant difference in the predictions - instead we find the outputs are similar. We review literature from the social sciences on anthropomorphization, and how both the methods and public image of AI are ideal for the misinterpretation of output as 'understanding'. We call for more caution in the interpretations of research results from the academic community. 
</div>

![](images/attack_all_measures.png)

<div style="text-align: center;">
## Annotation Practices in Societally Impactful Machine Learning Applications: What are Popular Recommender Systems Models Actually Trained On?
</div>

[Workshop: Perspectives on the Evaluation of Recommender Systems, 2023](https://ceur-ws.org/Vol-3476/paper1.pdf)

<div style="text-align: justify;"> Machine learning (ML) models play a big role in systems that help people make decisions - like what to buy, read, or watch, especially for massive repositories of content. These systems are trained on large datasets, but there’s no consistent way people collect data and report the process by which it collected or labeled. In this study, we looked at the 100 most-cited papers on recommender systems from the most highly cited conferences and journals, to see what they report about the data they use in their work. We found that important details - like who did the labeling, what quality checks were done, or whether the data is publicly available - are often missing. Many papers rely on a small set of benchmark datasets, with missing or otherwise very little information on how the data was originally gathered. This makes it hard to assess or reproduce results. We argue that future work should pay more attention to data quality and transparency, not just model performance, especially given the social impact of these systems. </div>

<div style="text-align: center;">
## Alexandria: A proof-of-concept micropublication platform
</div>
::::: {.columns .responsive-columns}

::: {.column width="50%"}

[Website](https://alexandria.ewi.tudelft.nl/about)  · [Open Science Framework](https://osf.io/hd5nu/) · [<i class="bi bi-github"></i>](https://github.com/Alexandria-TEMP)

<div style="text-align: justify;">
Alexandria is a design for a collaborative open-source platform for publishing, discussing, and developing scientific research. It was designed in conversation with Cynthia Liem's lab at TU Delft, and two software bachelor student groups, and one masters UX design student. The aim was to combine version control (git) with an intuitive, community-driven interface inspired by wikipedia. Users can share reflections, ask questions, or publish work. 

The definition of 'work' is broad, and includes data, code, or any other relevant material. Everything remains editable, and can be copied and changed by the community: inspired by Wikipedia, it encourages micro-publication, and collaboration on shared materials similar to articles - but with a focus on the narrow topics and additional materials needed for science. Changes are proposed and reviewed by peers with relevant expertise - although formally defining this process would require further study. Posts themselves are Quarto repositories rendered into readable html pages, bridging the gap between code and final research outputs. 

</div>


:::

::: {.column width="50%" style="padding-left: 1rem;"}
<div style="padding: 0.25rem; border-radius: 0.25rem; margin-bottom: 0.5rem; margin-top:3rem;">
![](images/logo32white.svg){width="90%" style="float: right; margin-bottom: 0.25rem;"}
</div>

:::

:::::

<div style="text-align: center;">
## Psychology Meets Machine Learning: Interdisciplinary Perspectives on Algorithmic Job Candidate Screening
</div>

[Book: Explainable and Interpretable Models in Computer Vision and Machine Learning, 2018](https://link.springer.com/chapter/10.1007/978-3-319-98131-4_9)

<div style="text-align: justify;"> As machine learning is used more often in areas that affect people directly, its role in hiring processes, like job candidate screening, is growing. Automated, data-driven assessment offers scalability and consistency, but it can only be trusted if it’s transparent and explainable. This chapter was developed through ongoing conversations between psychologists and computer scientists, and explores job candidate screening from both perspectives. We begin with an overview of key research practices in psychology and machine learning, then compare how each field approaches assessment. A use case is presented: a machine learning-based screening system designed to be understandable for non-technical hiring professionals. The chapter closes with practical advice for building stronger collaborations between technical and social science fields on this important topic. </div>


<div style="text-align: center;">
### Check out my other work on <span class="bi bi-google"></span> [Scholar](https://scholar.google.nl/citations?user=TFnVkFQAAAAJ&hl=en)
</div>
